{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import csv\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "import librosa\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, Conv2D, MaxPooling2D, Dropout, Activation, Flatten, Bidirectional\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "import datetime\n",
    "\n",
    "%run math_utils.py\n",
    "%run plot_utils.py\n",
    "%run load_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=16)\n",
    "plt.rc('figure', figsize=(20.0, 15.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = load_metadata('metadata/UrbanSound8K.csv', base_path='./UrbanSound8K')\n",
    "metadata['class_label'] = int_to_one_hot(metadata['class_id'], len(metadata['class_dict']))\n",
    "\n",
    "folds_file = \"folds.txt\"\n",
    "n_fold = 5\n",
    "\n",
    "# load folds file\n",
    "# create if not exist\n",
    "if not os.path.exists(folds_file): \n",
    "    data_len = len(metadata['fold'])\n",
    "    folds = np.concatenate([np.repeat(i, data_len // n_fold) for i in range(1, n_fold + 1)])\n",
    "    folds = np.append(folds, (np.random.randint(1, n_fold+1, data_len % n_fold)))\n",
    "    np.random.shuffle(folds)\n",
    "    assert len(folds) == len(metadata['fold'])\n",
    "    with open(folds_file, \"w\") as f:\n",
    "        for s in folds:\n",
    "            f.write(str(s) +\"\\n\")\n",
    "\n",
    "metadata['alt_fold'] = []\n",
    "with open(folds_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        metadata['alt_fold'].append(int(line.strip()))\n",
    "        \n",
    "u, c = np.unique(metadata['alt_fold'], return_counts=True)\n",
    "print(\"Folds:\", u, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_stft_callback(s):\n",
    "    return  librosa.feature.melspectrogram(\n",
    "        s, 44100,\n",
    "        n_fft=1024,\n",
    "        hop_length=1024,\n",
    "        n_mels=128,\n",
    "        power=1\n",
    "    )\n",
    "\n",
    "data_tag = 'stft_1024_1024_1024_44100_mel_128_pow_1'\n",
    "\n",
    "data = load_large_data(\n",
    "    metadata, 16,\n",
    "    callback=mel_stft_callback, args=(), sample_rate=44100,\n",
    "    cache_tag=data_tag, base_path='./UrbanSound8K',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "labels = np.array(metadata['class_label']) \n",
    "\n",
    "class_dict = metadata['class_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "def preprocess(item):\n",
    "    mean = np.mean(item)\n",
    "    std = np.std(item)\n",
    "    return (item - mean) / std\n",
    "\n",
    "data = [preprocess(item) for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preview\n",
    "sample = data[1] \n",
    "\n",
    "print(np.shape(sample))\n",
    "print(np.sum(sample ** 2))\n",
    "print(np.max(sample))\n",
    "print(np.min(sample))\n",
    "\n",
    "plt.imshow(sample, aspect='auto',interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take frame from sequence\n",
    "def random_clip(item):\n",
    "    signal_length = len(item[1])\n",
    "    difference = signal_length - 128 \n",
    "    if difference > 0:\n",
    "        shift = np.random.randint(0, difference)\n",
    "        return item[:, shift:shift+128]\n",
    "    if difference < 0: \n",
    "        difference = np.abs(difference)      \n",
    "        pad_left = np.random.randint(0, difference)\n",
    "        pad_right = difference - pad_left\n",
    "        return np.pad(item, ((0, 0), (pad_left, pad_right)), 'constant', constant_values=0)\n",
    "    return item\n",
    "\n",
    "# data for cnn, 128x128\n",
    "cnn_data = (list(map(random_clip, data)))\n",
    "cnn_labels = labels\n",
    "\n",
    "# data for lstm as is, 128xT\n",
    "lstm_data = data\n",
    "lstm_labels = labels\n",
    "\n",
    "assert len(data) == len(labels)\n",
    "print(len(data))\n",
    "\n",
    "ctag = 'full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test \n",
    "def split_data(metadata, test_fold, alt=False):\n",
    "    folds = metadata['fold'] if not alt else metadata['alt_fold']\n",
    "    assert len(folds) == len(data)\n",
    "    train, test = [], []\n",
    "    for i in np.random.permutation(len(data)):\n",
    "        if folds[i] in test_fold:\n",
    "            test.append(i)\n",
    "        else:\n",
    "            train.append(i)\n",
    "    return np.array(train), np.array(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save confusion matrix in csv\n",
    "def save_confusion_matrix(metadata, predict, data, labels, test_indices, train_indices, path):\n",
    "\n",
    "    assert len(data) == len(predict)\n",
    "    assert len(data) == len(labels)\n",
    "    assert len(train_indices) + len(test_indices) == len(data)\n",
    "\n",
    "    predict = (predict == np.max(predict, axis=1)[:, np.newaxis]).astype(dtype=int)\n",
    "    \n",
    "    class_dict = metadata['class_dict']\n",
    "    \n",
    "    labels_array = np.array(labels)\n",
    "    test_cm = confusion_matrix(labels_array[test_indices], predict[test_indices], len(class_dict))\n",
    "    train_cm = confusion_matrix(labels_array[train_indices], predict[train_indices], len(class_dict))\n",
    "    all_cm = confusion_matrix(labels_array, predict, len(class_dict))\n",
    "    \n",
    "    with open(path + '_test_cm.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(test_cm.tolist())\n",
    "    \n",
    "    with open(path + '_train_cm.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(train_cm.tolist())\n",
    "    \n",
    "    with open(path + '_all_cm.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(all_cm.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    metadata,\n",
    "    data,\n",
    "    labels,\n",
    "    test_fold,\n",
    "    tag,\n",
    "    save_path,\n",
    "    generator_callback=(lambda a, b: (a, b))\n",
    "):\n",
    "    \n",
    "    train_indices, test_indices = split_data(metadata, test_fold, alt=True)\n",
    "    \n",
    "    assert len(set(train_indices) & set(test_indices)) == 0\n",
    "    assert (set(train_indices) | set(test_indices)) == set(range(len(data)))\n",
    "    assert len(data) == len(labels)\n",
    "    assert len(train_indices) + len(test_indices) == len(data)\n",
    "\n",
    "    def train_generator():\n",
    "        while True:\n",
    "            for i in train_indices:\n",
    "                yield generator_callback(data[i], labels[i])\n",
    "        \n",
    "    def test_generator():\n",
    "        while True:\n",
    "            for i in test_indices:\n",
    "                yield generator_callback(data[i], labels[i])\n",
    "                \n",
    "    def all_data_generator():\n",
    "        while True:\n",
    "            for i in range(len(data)):\n",
    "                yield generator_callback(data[i], labels[i])\n",
    "    \n",
    "    btag = tag + ('_tf%s_' % \"_\".join(str(i) for i in test_fold)) + \\\n",
    "                                            datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "\n",
    "    checkpoint_format = 'model.best-acc.hdf5'\n",
    "    checkpoint_file = save_path + btag + '_' + checkpoint_format\n",
    "\n",
    "    checkpoint = ModelCheckpoint(checkpoint_file, verbose=0, monitor='val_acc', save_best_only=True, mode='max')\n",
    "    board = TensorBoard(save_path + btag, write_graph=True, write_grads=True, write_images=True)\n",
    "      \n",
    "    history = model.fit_generator(\n",
    "        train_generator(), steps_per_epoch=len(train_indices), epochs=64,\n",
    "        validation_data=test_generator(), validation_steps=len(test_indices),\n",
    "        callbacks=[board, checkpoint], verbose=1\n",
    "    )\n",
    "    \n",
    "    model.load_weights(checkpoint_file)\n",
    "    \n",
    "    predict = model.predict_generator(all_data_generator(), len(data))\n",
    "    save_confusion_matrix(metadata, predict, data, labels, test_indices, train_indices, save_path + btag)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(24, (5, 5), input_shape=(128, 128, 1)))\n",
    "    model.add(MaxPooling2D((4, 2), (4, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "          \n",
    "    model.add(Conv2D(48, (5, 5)))\n",
    "    model.add(MaxPooling2D((4, 2), (4, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "          \n",
    "    model.add(Conv2D(48, (5, 5)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    \n",
    "tag = 'cnn_adam_' + data_tag + '_' + ctag\n",
    "log_path = './logs_cv_cnn_mel_p1_norm/'\n",
    "\n",
    "shape_callback = lambda d, l: (d.T[np.newaxis, :, :, np.newaxis], l[np.newaxis, :])\n",
    "\n",
    "best_test_acc = []\n",
    "\n",
    "for i in range(1, 6, 1): \n",
    "    \n",
    "    model = cnn_model()\n",
    "    \n",
    "    h = train_model(\n",
    "        model, metadata,\n",
    "        cnn_data, cnn_labels,\n",
    "        test_fold=[i], tag=tag,\n",
    "        save_path=log_path,\n",
    "        generator_callback=shape_callback\n",
    "    ).history\n",
    "    \n",
    "    best_acc = np.max(h['val_acc'])\n",
    "    best_test_acc.append(best_acc)\n",
    "    print(best_acc)\n",
    "\n",
    "with open(log_path +tag + '_result.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([best_test_acc])\n",
    "    \n",
    "\n",
    "print(best_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lstm_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(None, 128)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.25)) \n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    \n",
    "tag = 'lstm_adam_' + data_tag + '_' + ctag\n",
    "log_path = './logs_cv_lstm_mel_p1_norm/'\n",
    "\n",
    "shape_callback = lambda d, l: (d.T[np.newaxis, :, :], l[np.newaxis, :])\n",
    "\n",
    "best_test_acc = []\n",
    "\n",
    "for i in range(2, 6, 1): \n",
    "    \n",
    "    model = lstm_model()\n",
    "    \n",
    "    h = train_model(\n",
    "        model, metadata,\n",
    "        lstm_data, lstm_labels,\n",
    "        test_fold=[i], tag=tag,\n",
    "        save_path=log_path,\n",
    "        generator_callback=shape_callback\n",
    "    ).history\n",
    "    \n",
    "    best_acc = np.max(h['val_acc'])\n",
    "    best_test_acc.append(best_acc)\n",
    "    print(best_acc)\n",
    "\n",
    "print(best_test_acc)\n",
    "\n",
    "with open(log_path + tag + '_result.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([best_test_acc])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
